{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6342433-152f-420f-9b28-361c2d4a9a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29c0f85a-b423-4ebf-aa88-02681601b1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"stefanoleone992/mutual-funds-and-etfs\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728d2f45-eaa3-4d2c-aa46-38c39c78f9b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Local path where kagglehub downloaded files\n",
    "local_path = Path(\"/root/.cache/kagglehub/datasets/stefanoleone992/mutual-funds-and-etfs/versions/4\")\n",
    "\n",
    "# Target path in DBFS\n",
    "dbfs_path = \"dbfs:/tmp/bronze/mutual-funds-and-etfs\"\n",
    "\n",
    "# Copy each file to DBFS\n",
    "for file in glob.glob(str(local_path / \"*\")):\n",
    "    file_name = os.path.basename(file)\n",
    "    dbutils.fs.cp(f\"file:{file}\", f\"{dbfs_path}/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575be40e-5d04-4a0b-a6b7-d1ca80e16f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List local\n",
    "# os.listdir(\"/root/.cache/kagglehub/datasets/whenamancodes/fraud-detection/versions/1\")\n",
    "\n",
    "# List DBFS\n",
    "display(dbutils.fs.ls(\"dbfs:/tmp/bronze/mutual-funds-and-etfs/\"))\n",
    "# dbutils.fs.rm(\"dbfs:/_delta_log\", recurse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59bcf877-6e36-470a-8583-274b362ef6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Context\n",
    "\n",
    "ETFs represent a cheap alternative to Mutual Funds and they are growing fast in the last years due to their passive approach (and the consequential lower fees).\n",
    "This dataset includes the financial information collected from Yahoo Finance and includes all U.S. Mutual Funds and along with their historical prices.\n",
    "Updated version relates to the November 2021 financial values.\n",
    "\n",
    "Content\n",
    "The file contains 23,783 Mutual Funds and 2,310 ETFs with:\n",
    "\n",
    "General fund aspects (e.g. total_net_assets, fund family, inception date, etc.)\n",
    "Portfolio indicators (e.g. cash, stocks, bonds, sectors, etc.)\n",
    "Historical yearly and quarterly returns (e.g. year_to_date, 1-year, 3-years, etc.)\n",
    "Financial ratios (price/earning, Treynor and Sharpe ratios, alpha, and beta)\n",
    "ESG scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b27edea-48e9-4774-a457-37a937994641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_data():\n",
    "    # Downloading the dataset from kaggle https://www.kaggle.com/datasets/stefanoleone992/mutual-funds-and-etfs/data\n",
    "    path = kagglehub.dataset_download(\"stefanoleone992/mutual-funds-and-etfs\")\n",
    "\n",
    "    #Moving dataset from root to DBFS\n",
    "    local_path = Path(path) #where the file is stored\n",
    "    \n",
    "    dbfs_path = \"dbfs:/tmp/bronze/mutual-funds-and-etfs\" # Target path in DBFS\n",
    "\n",
    "    for file in glob.glob(str(local_path / \"*\")): # Copy each file to DBFS\n",
    "        file_name = os.path.basename(file)\n",
    "        dbutils.fs.cp(f\"file:{file}\", f\"{dbfs_path}/{file_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02273e56-05d0-4e53-9de5-4e9c88693597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "paths = {\n",
    "    \"fund_a_e\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - A-E.csv\",\n",
    "    \"fund_f_k\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - F-K.csv\",\n",
    "    \"fund_l_p\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - L-P.csv\",\n",
    "    \"d_q_z\":   \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - Q-Z.csv\",\n",
    "    \"etf_prices\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/ETF prices.csv\",\n",
    "    \"etfs\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/ETFs.csv\",\n",
    "    \"funds\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFunds.csv\"\n",
    "}\n",
    "\n",
    "#The CSV file contains all historical prices available on YahooFinance for the 2,310 scraped ETFs\n",
    "etf_prices = spark.read.csv(paths['etf_prices'], header=True, inferSchema=True)\n",
    "del paths['etf_prices']\n",
    "\n",
    "# The CSV file contains data for 2,310 ETFs with 142 different attributes\n",
    "etfs = spark.read.csv(paths['etfs'], header=True, inferSchema=True)\n",
    "del paths['etfs']\n",
    "\n",
    "# The CSV file contains data for 23,783 Mutual Funds with 298 different attributes\n",
    "funds =  spark.read.csv(paths['funds'], header=True, inferSchema=True)\n",
    "del paths['funds']\n",
    "\n",
    "# These datasets are a list of funds with their fund code, date and price \n",
    "# The CSV file contains all historical prices available on YahooFinance for the scraped Mutual Funds with symbol starting with letters from A to E\n",
    "def load_csv(name, path):\n",
    "    return name, spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(lambda kv: load_csv(*kv), paths.items())\n",
    "\n",
    "# Collect results into named variables\n",
    "csv_dfs = dict(results)\n",
    "fund_prices = (\n",
    "    csv_dfs[\"fund_a_e\"]\n",
    "    .unionByName(csv_dfs[\"fund_f_k\"])\n",
    "    .unionByName(csv_dfs[\"fund_l_p\"])\n",
    "    .unionByName(csv_dfs[\"d_q_z\"])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4267cc39-31fd-4c49-be7f-0ef78c2313f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Datasets Discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45954857-c668-4d93-afa8-65d7f6517aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etc_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f524325-e12c-4625-9a4a-fb34e1ae09c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etc_prices.summary())\n",
    "\n",
    "# FINE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242f1175-474c-4014-97c9-143c24941c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c7f4b4-7be0-4db9-a063-e1545c8a6d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etfs.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6dfdd7-e5c4-4aeb-9067-5cc1bc94cd45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fund_prices) # THIS IS FINE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6ab1e0-6660-47da-9e86-4e15f561f985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fund_prices.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8c344-88d2-4db9-9dc5-28698a134de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(funds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc5bd6f-c7f6-430d-8901-a09625873df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(funds.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe231b29-0965-4772-9c18-1143924082ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Cleaning\n",
    "\n",
    "This dataset has many tables, and multiple columns. There's too many columns to handle the data cleaning process at once due to multiple possible of occurring. In order to solve this problem, we can apply the Medallion Architecture and break it down our problem. \n",
    "\n",
    "The Medallion Architecture relies on 3 layers, where the first one receives the loaded raw data, and this data is processed for the next layer (silver) already formally treated (thus not having nulls, and appropriate data type treatment). This layer is already ML-ready and can be used for further algorithms. \n",
    "\n",
    "At first we'd have to create a categorical table which summarizes most information of our tables, and then split these big tables into smaller ones and domain-specify, similar to Star-Schema design where we have some dimension tables connected to fact tables. \n",
    "\n",
    "This fact table could be the one in the \"golden\" layer equivalent, and in which is the table with the output of the model and a second golden-layer table would be a recreation of the raw data ingested but already properly treated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a76e996-995d-4d77-9239-1608fa1c7d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fund_prices.head(10)) #Seems already ok for silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9b80a0-31e4-43e2-a03a-d47528a9d6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fund_prices.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7772c9c-58d0-40c4-8b0b-c95b8369e22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etf_prices.head(10)) #They seem already ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcf0c6d-fea0-408e-8163-5dcb39b5c561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_date\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import time\n",
    "\n",
    "# Define function to clean, cast, add date, and write partitioned Parquet\n",
    "def clean_and_store(df: DataFrame, name: str, id_columns: list, output_path: str):\n",
    "    print(f\"Starting processing for group: {name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Remove rows where all non-ID columns are null\n",
    "    cols_to_check = [c for c in df.columns if c not in id_columns]\n",
    "    df = df.dropna(subset=cols_to_check, how=\"all\")\n",
    "\n",
    "    # Remove rows with string values in non-ID columns\n",
    "    for c in cols_to_check:\n",
    "        df = df.filter(~col(c).rlike(\"[^0-9.,-]\"))\n",
    "\n",
    "    # Convert non-ID columns to DoubleType\n",
    "    for c in cols_to_check:\n",
    "        df = df.withColumn(c, F.regexp_replace(F.col(c), ',', ''))\n",
    "        df = df.withColumn(c, F.when(F.col(c).isin(\"NULL\", \"\"), None).otherwise(F.col(c).cast(DoubleType())))\n",
    "\n",
    "    non_double_columns = [f.name for f in df.schema.fields if not isinstance(f.dataType, DoubleType) and f.name not in id_columns]\n",
    "\n",
    "    for column in non_double_columns:\n",
    "        desc_vals = df.select(column).distinct().orderBy(F.col(column).desc()).limit(1000).rdd.map(lambda r: str(r[0])).collect()\n",
    "        asc_vals = df.select(column).distinct().orderBy(F.col(column).asc()).limit(1000).rdd.map(lambda r: str(r[0])).collect()\n",
    "\n",
    "        def is_mixed(vals):\n",
    "            return any(not v.replace('.', '', 1).replace('-', '', 1).isdigit() for v in vals)\n",
    "\n",
    "        if is_mixed(desc_vals) or is_mixed(asc_vals):\n",
    "            # Separate rows with string-like values\n",
    "            string_rows = df.filter(F.col(column).rlike(\"[^0-9.,-]\"))\n",
    "            fund_prices = df.filter(~F.col(column).rlike(\"[^0-9.,-]\"))\n",
    "            fund_prices = df.withColumn(column, F.col(column).cast(DoubleType()))\n",
    "\n",
    "            print(f\"\\nColumn '{column}' contained mixed values. String rows moved to separate DataFrame.\")\n",
    "            string_rows.select(column).distinct().show()\n",
    "    \n",
    "\n",
    "    # Add insertion_date column to fit with good practices of data engineering\n",
    "    df = df.withColumn(\"insertion_date\", current_date())\n",
    "\n",
    "    # Repartition for parallel writing \n",
    "    # it boots write parallelism to DBFS and improves scalability during .write()\n",
    "    df = df.repartition(100)\n",
    "\n",
    "    # Cache if reused, it stores the dataframe in memory after transformations\n",
    "    # it prevents recomputation of previous steps and speeds up multiple actions\n",
    "    df.cache()\n",
    "\n",
    "    # Write to DBFS partitioned by insertion_date\n",
    "    try: \n",
    "        output_dir = f\"{output_path}/{name}/\"\n",
    "        df.write.mode(\"overwrite\").partitionBy(\"insertion_date\").parquet(output_dir)\n",
    "    except Exception as e:\n",
    "        print(f'Error Processing group {name}, Exception {e}')\n",
    "\n",
    "    df.unpersist()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished processing group: {name} in {end_time - start_time:.2f} seconds. Saved to {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74531324-378c-4848-93dc-c7f1cf83b500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "etfs = etfs.repartition(200).cache()\n",
    "etfs.count()  # to trigger caching immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a41a512-cbc6-4785-9d66-c40576b7096d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating CATALOG table \n",
    "\n",
    "# Identifying key columns for the 2 biggest dataframes: funds and etfs\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "\n",
    "# Define column groups by pattern\n",
    "join_keys = ['fund_symbol', 'exchange_code']\n",
    "\n",
    "# Group 1: Identification\n",
    "id_columns = join_keys + [\n",
    "    'quote_type', 'region', 'fund_short_name', 'fund_long_name', 'currency',\n",
    "    'fund_category', 'fund_family', 'exchange_name', 'exchange_timezone'\n",
    "]\n",
    "\n",
    "# Group 2: Volume and Assets\n",
    "volume_columns = join_keys + [\n",
    "    'avg_vol_3month', 'avg_vol_10day', 'total_net_assets'\n",
    "]\n",
    "\n",
    "# Group 3: Moving Averages and High/Low Stats\n",
    "trend_columns = join_keys + [\n",
    "    'day50_moving_average', 'day200_moving_average', 'week52_high_low_change',\n",
    "    'week52_high_low_change_perc', 'week52_high', 'week52_high_change',\n",
    "    'week52_high_change_perc', 'week52_low', 'week52_low_change', 'week52_low_change_perc'\n",
    "]\n",
    "\n",
    "# Group 4: Strategy & Yield\n",
    "strategy_columns = join_keys + [\n",
    "    'investment_strategy', 'fund_yield', 'inception_date', 'annual_holdings_turnover',\n",
    "    'investment_type', 'size_type'\n",
    "]\n",
    "\n",
    "# Group 5: Expense Ratios\n",
    "expense_columns = join_keys + [\n",
    "    'fund_annual_report_net_expense_ratio', 'category_annual_report_net_expense_ratio'\n",
    "]\n",
    "\n",
    "# Group 6: Asset Allocation\n",
    "asset_columns = join_keys + [\n",
    "    'asset_stocks', 'asset_bonds'\n",
    "]\n",
    "\n",
    "# Group 7: Sectors\n",
    "sector_columns = join_keys + [\n",
    "    'fund_sector_basic_materials', 'fund_sector_communication_services',\n",
    "    'fund_sector_consumer_cyclical', 'fund_sector_consumer_defensive',\n",
    "    'fund_sector_energy', 'fund_sector_financial_services', 'fund_sector_healthcare',\n",
    "    'fund_sector_industrials', 'fund_sector_real_estate', 'fund_sector_technology',\n",
    "    'fund_sector_utilities'\n",
    "]\n",
    "\n",
    "# Group 8: Valuation Ratios\n",
    "valuation_columns = join_keys + [\n",
    "    'fund_price_book_ratio', 'fund_price_cashflow_ratio', 'fund_price_earning_ratio',\n",
    "    'fund_price_sales_ratio'\n",
    "]\n",
    "\n",
    "# Group 9: Bond Ratings\n",
    "bond_columns = join_keys + [\n",
    "    'fund_bond_maturity', 'fund_bond_duration', 'fund_bonds_us_government', 'fund_bonds_aaa',\n",
    "    'fund_bonds_aa', 'fund_bonds_a', 'fund_bonds_bbb', 'fund_bonds_bb', 'fund_bonds_b',\n",
    "    'fund_bonds_below_b', 'fund_bonds_others'\n",
    "]\n",
    "\n",
    "# Group 10: Holdings\n",
    "holding_columns = join_keys + [\n",
    "    'top10_holdings', 'top10_holdings_total_assets'\n",
    "]\n",
    "\n",
    "# Group 11: Returns - Periods\n",
    "return_columns = join_keys + [\n",
    "    'returns_as_of_date', 'fund_return_ytd', 'category_return_ytd', 'fund_return_1month',\n",
    "    'category_return_1month', 'fund_return_3months', 'category_return_3months',\n",
    "    'fund_return_1year', 'category_return_1year', 'fund_return_3years', 'category_return_3years',\n",
    "    'fund_return_5years', 'category_return_5years', 'fund_return_10years', 'category_return_10years',\n",
    "    'years_up', 'years_down'\n",
    "]\n",
    "\n",
    "# Group 12: Returns - Annual\n",
    "annual_return_columns = join_keys + [\n",
    "    f'fund_return_{y}' for y in range(2020, 1999, -1)\n",
    "] + [\n",
    "    f'category_return_{y}' for y in range(2020, 1999, -1)\n",
    "]\n",
    "\n",
    "# Group 13: Risk Metrics\n",
    "risk_columns = join_keys + [\n",
    "    'fund_alpha_3years', 'fund_beta_3years', 'fund_mean_annual_return_3years',\n",
    "    'fund_r_squared_3years', 'fund_stdev_3years', 'fund_sharpe_ratio_3years',\n",
    "    'fund_treynor_ratio_3years', 'fund_alpha_5years', 'fund_beta_5years',\n",
    "    'fund_mean_annual_return_5years', 'fund_r_squared_5years', 'fund_stdev_5years',\n",
    "    'fund_sharpe_ratio_5years', 'fund_treynor_ratio_5years', 'fund_alpha_10years',\n",
    "    'fund_beta_10years', 'fund_mean_annual_return_10years', 'fund_r_squared_10years',\n",
    "    'fund_stdev_10years', 'fund_sharpe_ratio_10years', 'fund_treynor_ratio_10years'\n",
    "]\n",
    "\n",
    "# Group DataFrames\n",
    "group_dfs_etfs = {\n",
    "    \"identification\": etfs.select(*id_columns),\n",
    "    \"volume\": etfs.select(*volume_columns),\n",
    "    \"trend\": etfs.select(*trend_columns),\n",
    "    \"strategy\": etfs.select(*strategy_columns),\n",
    "    \"expenses\": etfs.select(*expense_columns),\n",
    "    \"assets\": etfs.select(*asset_columns),\n",
    "    \"sectors\": etfs.select(*sector_columns),\n",
    "    \"valuation\": etfs.select(*valuation_columns),\n",
    "    \"bonds\": etfs.select(*bond_columns),\n",
    "    \"holdings\": etfs.select(*holding_columns),\n",
    "    \"returns_period\": etfs.select(*return_columns),\n",
    "    \"returns_annual\": etfs.select(*annual_return_columns),\n",
    "    \"risk\": etfs.select(*risk_columns)\n",
    "}\n",
    "\n",
    "print(f\"Created {len(group_dfs_etfs)} grouped DataFrames by column pattern.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a72c1d60-aa70-4c52-bf1a-6b412641d164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply function to each DataFrame\n",
    "dbfs_output = \"dbfs:/tmp/silver/etfs\"\n",
    "\n",
    "# for name, df in group_dfs_etfs.items():\n",
    "#     clean_and_store(df, name, id_columns[:2], dbfs_output)\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(clean_and_store, df, name, id_columns[:2], dbfs_output)\n",
    "        for name, df in group_dfs_etfs.items()\n",
    "    ]\n",
    "    for future in futures:\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1e78ef-f187-4865-9a5d-8a9afa35df67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etfs.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d35ac4-96d3-439a-bea9-311547355248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "funds = funds.repartition(200).cache()\n",
    "funds.count()  # to trigger caching immediately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fdaf25e-93e2-464b-ae3c-3df4a9545dfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Join keys\n",
    "join_keys = ['fund_symbol', 'exchange_code']\n",
    "\n",
    "# Define relevant groups based on pattern matching or logical grouping\n",
    "id_columns = join_keys + [\n",
    "    'quote_type', 'region', 'fund_short_name', 'fund_long_name', 'currency',\n",
    "    'initial_investment', 'subsequent_investment', 'fund_category', 'fund_family',\n",
    "    'exchange_name', 'exchange_timezone', 'management_name', 'management_bio',\n",
    "    'management_start_date'\n",
    "]\n",
    "\n",
    "volume_columns = join_keys + ['total_net_assets', 'year_to_date_return']\n",
    "\n",
    "trend_columns = join_keys + [\n",
    "    'day50_moving_average', 'day200_moving_average', 'week52_high_low_change',\n",
    "    'week52_high_low_change_perc', 'week52_high', 'week52_high_change',\n",
    "    'week52_high_change_perc', 'week52_low', 'week52_low_change', 'week52_low_change_perc'\n",
    "]\n",
    "\n",
    "# DISCARD\n",
    "strategy_columns = join_keys + [\n",
    "    'investment_strategy', 'fund_yield', 'morningstar_overall_rating', 'morningstar_risk_rating',\n",
    "    'inception_date', 'last_dividend', 'last_cap_gain', 'annual_holdings_turnover',\n",
    "    'investment_type', 'size_type'\n",
    "]\n",
    "\n",
    "expense_columns = join_keys + [\n",
    "    'fund_annual_report_net_expense_ratio', 'category_annual_report_net_expense_ratio',\n",
    "    'fund_prospectus_net_expense_ratio', 'fund_prospectus_gross_expense_ratio', 'fund_max_12b1_fee',\n",
    "    'fund_max_front_end_sales_load', 'category_max_front_end_sales_load', 'fund_max_deferred_sales_load',\n",
    "    'category_max_deferred_sales_load', 'fund_year3_expense_projection', 'fund_year5_expense_projection',\n",
    "    'fund_year10_expense_projection'\n",
    "]\n",
    "\n",
    "asset_columns = join_keys + [\n",
    "    'asset_cash', 'asset_stocks', 'asset_bonds', 'asset_others', 'asset_preferred', 'asset_convertible'\n",
    "]\n",
    "\n",
    "sector_columns = join_keys + [\n",
    "    'fund_sector_basic_materials', 'fund_sector_communication_services', 'fund_sector_consumer_cyclical',\n",
    "    'fund_sector_consumer_defensive', 'fund_sector_energy', 'fund_sector_financial_services',\n",
    "    'fund_sector_healthcare', 'fund_sector_industrials', 'fund_sector_real_estate',\n",
    "    'fund_sector_technology', 'fund_sector_utilities'\n",
    "]\n",
    "\n",
    "valuation_columns = join_keys + [\n",
    "    'fund_price_book_ratio', 'category_price_book_ratio', 'fund_price_cashflow_ratio',\n",
    "    'category_price_cashflow_ratio', 'fund_price_earning_ratio', 'category_price_earning_ratio',\n",
    "    'fund_price_sales_ratio', 'category_price_sales_ratio', 'fund_median_market_cap',\n",
    "    'category_median_market_cap', 'fund_year3_earnings_growth', 'category_year3_earnings_growth'\n",
    "]\n",
    "\n",
    "bond_columns = join_keys + [\n",
    "    'fund_bond_maturity', 'category_bond_maturity', 'fund_bond_duration', 'category_bond_duration',\n",
    "    'fund_bonds_us_government', 'fund_bonds_aaa', 'fund_bonds_aa', 'fund_bonds_a',\n",
    "    'fund_bonds_bbb', 'fund_bonds_bb', 'fund_bonds_b', 'fund_bonds_below_b', 'fund_bonds_others'\n",
    "]\n",
    "\n",
    "holding_columns = join_keys + [\n",
    "    'top10_holdings', 'top10_holdings_total_assets'\n",
    "]\n",
    "\n",
    "return_columns = join_keys + [\n",
    "    'morningstar_return_rating', 'returns_as_of_date', 'fund_return_ytd', 'category_return_ytd',\n",
    "    'fund_return_1month', 'category_return_1month', 'fund_return_3months', 'category_return_3months',\n",
    "    'fund_return_1year', 'category_return_1year', 'fund_return_3years', 'category_return_3years',\n",
    "    'fund_return_5years', 'category_return_5years', 'fund_return_10years', 'category_return_10years',\n",
    "    'fund_return_last_bull_market', 'category_return_last_bull_market',\n",
    "    'fund_return_last_bear_market', 'category_return_last_bear_market', 'years_up', 'years_down'\n",
    "]\n",
    "\n",
    "annual_return_columns = join_keys + [\n",
    "    f'fund_return_{y}' for y in range(2020, 1999, -1)\n",
    "] + [\n",
    "    f'category_return_{y}' for y in range(2020, 1999, -1)\n",
    "]\n",
    "\n",
    "quarter_return_columns = join_keys + [\n",
    "    f'fund_return_{y}_q{q}' for y in range(2021, 1999, -1) for q in range(4, 0, -1)\n",
    "]\n",
    "quarter_return_columns.remove('fund_return_2021_q4')\n",
    "\n",
    "risk_columns = join_keys + [\n",
    "    'fund_alpha_3years', 'fund_beta_3years', 'fund_mean_annual_return_3years', 'fund_r_squared_3years',\n",
    "    'fund_stdev_3years', 'fund_sharpe_ratio_3years', 'fund_treynor_ratio_3years',\n",
    "    'fund_alpha_5years', 'fund_beta_5years', 'fund_mean_annual_return_5years', 'fund_r_squared_5years',\n",
    "    'fund_stdev_5years', 'fund_sharpe_ratio_5years', 'fund_treynor_ratio_5years',\n",
    "    'fund_alpha_10years', 'fund_beta_10years', 'fund_mean_annual_return_10years',\n",
    "    'fund_r_squared_10years', 'fund_stdev_10years', 'fund_sharpe_ratio_10years', 'fund_treynor_ratio_10years'\n",
    "]\n",
    "\n",
    "rank_columns = join_keys + [\n",
    "    'fund_return_category_rank_ytd', 'fund_return_category_rank_1month', 'fund_return_category_rank_3months',\n",
    "    'fund_return_category_rank_1year', 'fund_return_category_rank_3years', 'fund_return_category_rank_5years',\n",
    "    'load_adj_return_1year', 'load_adj_return_3years', 'load_adj_return_5years', 'load_adj_return_10years'\n",
    "]\n",
    "\n",
    "sustainability_columns = join_keys + [\n",
    "    'sustainability_score', 'sustainability_rank', 'esg_peer_group', 'esg_peer_count', 'esg_score',\n",
    "    'peer_esg_min', 'peer_esg_avg', 'peer_esg_max', 'environment_score', 'peer_environment_min',\n",
    "    'peer_environment_avg', 'peer_environment_max', 'social_score', 'peer_social_min',\n",
    "    'peer_social_avg', 'peer_social_max', 'governance_score', 'peer_governance_min',\n",
    "    'peer_governance_avg', 'peer_governance_max'\n",
    "]\n",
    "\n",
    "# List of grouped DataFrames\n",
    "group_dfs = {\n",
    "    \"identification\": funds.select(*id_columns),\n",
    "    \"volume\": funds.select(*volume_columns),\n",
    "    \"trend\": funds.select(*trend_columns),\n",
    "    \"expenses\": funds.select(*expense_columns),\n",
    "    \"assets\": funds.select(*asset_columns),\n",
    "    \"sectors\": funds.select(*sector_columns),\n",
    "    \"valuation\": funds.select(*valuation_columns),\n",
    "    \"bonds\": funds.select(*bond_columns),\n",
    "    \"holdings\": funds.select(*holding_columns),\n",
    "    \"returns_period\": funds.select(*return_columns),\n",
    "    \"returns_annual\": funds.select(*annual_return_columns),\n",
    "    \"returns_quarter\":funds.select(*quarter_return_columns),\n",
    "    \"risk\": funds.select(*risk_columns),\n",
    "    \"rank\":funds.select(*rank_columns),\n",
    "    \"sustainability\":funds.select(*sustainability_columns)\n",
    "}\n",
    "\n",
    "print(f\"Created {len(group_dfs)} grouped DataFrames by column pattern.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bf7afb-836a-4302-9270-3404a53036f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply function to each DataFrame\n",
    "dbfs_output = \"dbfs:/tmp/silver/funds\"\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(clean_and_store, df, name, id_columns[:2], dbfs_output)\n",
    "        for name, df in group_dfs.items()\n",
    "    ]\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d022b6a5-171c-4126-a8e5-918bba51d882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "funds.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42120c12-b114-4369-96b9-f3491c6c0e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "display(dbutils.fs.ls(\"dbfs:/tmp/silver/funds/assets/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962f774a-df7d-4ca5-b008-d81210c04dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/tmp/silver/etfs/\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3b653bd-5a1d-45c8-b911-e857961a45c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Final Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f763feed-eaa1-4b1f-94d8-2f6625ed67e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/tmp/bronze/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5333b7-4065-4b38-8a78-78d088b92011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./funds_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dd2fd97-b4f4-46c4-b550-883cfeb254a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./etfs_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00e9a1f-cf40-4300-93ce-30913116ffca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Cleaning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.paths_ingest = {\n",
    "            \"fund_a_e\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - A-E.csv\",\n",
    "            \"fund_f_k\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - F-K.csv\",\n",
    "            \"fund_l_p\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - L-P.csv\",\n",
    "            \"d_q_z\":   \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFund prices - Q-Z.csv\",\n",
    "            \"etf_prices\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/ETF prices.csv\",\n",
    "            \"etfs\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/ETFs.csv\",\n",
    "            \"funds\": \"dbfs:/tmp/bronze/mutual-funds-and-etfs/MutualFunds.csv\"\n",
    "        }\n",
    "\n",
    "    def import_data(self):\n",
    "        # Downloading the dataset from kaggle https://www.kaggle.com/datasets/stefanoleone992/mutual-funds-and-etfs/data\n",
    "        path = kagglehub.dataset_download(\"stefanoleone992/mutual-funds-and-etfs\")\n",
    "\n",
    "        #Moving dataset from root to DBFS\n",
    "        local_path = Path(path) #where the file is stored\n",
    "        \n",
    "        dbfs_path = \"dbfs:/tmp/bronze/mutual-funds-and-etfs\" # Target path in DBFS\n",
    "\n",
    "        for file in glob.glob(str(local_path / \"*\")): # Copy each file to DBFS\n",
    "            file_name = os.path.basename(file)\n",
    "            dbutils.fs.cp(f\"file:{file}\", f\"{dbfs_path}/{file_name}\")\n",
    "        \n",
    "\n",
    "    def ingest_data(self):\n",
    "\n",
    "        #The CSV file contains all historical prices available on YahooFinance for the 2,310 scraped ETFs\n",
    "        etf_prices = spark.read.csv(self.paths_ingest['etf_prices'], header=True, inferSchema=True)\n",
    "        del self.paths_ingest['etf_prices']\n",
    "\n",
    "        # The CSV file contains data for 2,310 ETFs with 142 different attributes\n",
    "        etfs = spark.read.csv(self.paths_ingest['etfs'], header=True, inferSchema=True)\n",
    "        del self.paths_ingest['etfs']\n",
    "\n",
    "        # The CSV file contains data for 23,783 Mutual Funds with 298 different attributes\n",
    "        funds =  spark.read.csv(self.paths_ingest['funds'], header=True, inferSchema=True)\n",
    "        del self.paths_ingest['funds']\n",
    "        # These datasets are a list of funds with their fund code, date and price \n",
    "        # The CSV file contains all historical prices available on YahooFinance for the scraped Mutual Funds with symbol starting with letters from A to E\n",
    "        def load_csv(name, path):\n",
    "            return name, spark.read.csv(path, header=True, inferSchema=True)\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = executor.map(lambda kv: load_csv(*kv), paths.items())\n",
    "\n",
    "        # Collect results into named variables\n",
    "        csv_dfs = dict(results)\n",
    "        fund_prices = (\n",
    "            csv_dfs[\"fund_a_e\"]\n",
    "            .unionByName(csv_dfs[\"fund_f_k\"])\n",
    "            .unionByName(csv_dfs[\"fund_l_p\"])\n",
    "            .unionByName(csv_dfs[\"d_q_z\"])\n",
    "        )\n",
    "\n",
    "        return etf_prices, etfs, funds, fund_prices\n",
    "    \n",
    "    def transform(self,df, pipe):\n",
    "\n",
    "        if pipe == 'funds':\n",
    "\n",
    "            funds_quarter_return_columns = join_keys + [\n",
    "                f'fund_return_{y}_q{q}' for y in range(2021, 1999, -1) for q in range(4, 0, -1)\n",
    "            ]\n",
    "            funds_quarter_return_columns.remove('fund_return_2021_q4')\n",
    "\n",
    "            # List of grouped DataFrames\n",
    "            group_dfs = {\n",
    "                \"identification\": funds.select(*funds_id_columns),\n",
    "                \"volume\": funds.select(*funds_volume_columns),\n",
    "                \"trend\": funds.select(*funds_trend_columns),\n",
    "                \"expenses\": funds.select(*funds_expense_columns),\n",
    "                \"assets\": funds.select(*funds_asset_columns),\n",
    "                \"sectors\": funds.select(*funds_sector_columns),\n",
    "                \"valuation\": funds.select(*funds_valuation_columns),\n",
    "                \"bonds\": funds.select(*funds_bond_columns),\n",
    "                \"holdings\": funds.select(*funds_holding_columns),\n",
    "                \"returns_period\": funds.select(*funds_return_columns),\n",
    "                \"returns_annual\": funds.select(*funds_annual_return_columns),\n",
    "                \"returns_quarter\":funds.select(*funds_quarter_return_columns),\n",
    "                \"risk\": funds.select(*funds_risk_columns),\n",
    "                \"rank\":funds.select(*funds_rank_columns),\n",
    "                \"sustainability\":funds.select(*funds_sustainability_columns)\n",
    "            }\n",
    "\n",
    "            print(f\"Created {len(group_dfs)} grouped DataFrames by column pattern for FUNDS.\")\n",
    "\n",
    "            return group_dfs\n",
    "        elif pipe == 'etfs':\n",
    "\n",
    "            # Group 12: Returns - Annual\n",
    "            etf_annual_return_columns = join_keys + [\n",
    "                f'fund_return_{y}' for y in range(2020, 1999, -1)\n",
    "            ] + [\n",
    "                f'category_return_{y}' for y in range(2020, 1999, -1)\n",
    "            ]\n",
    "\n",
    "            # Group DataFrames\n",
    "            group_dfs = {\n",
    "                \"identification\": etfs.select(*etf_id_columns),\n",
    "                \"volume\": etfs.select(*etf_volume_columns),\n",
    "                \"trend\": etfs.select(*etf_trend_columns),\n",
    "                \"strategy\": etfs.select(*etf_strategy_columns),\n",
    "                \"expenses\": etfs.select(*etf_expense_columns),\n",
    "                \"assets\": etfs.select(*etf_asset_columns),\n",
    "                \"sectors\": etfs.select(*etf_sector_columns),\n",
    "                \"valuation\": etfs.select(*etf_valuation_columns),\n",
    "                \"bonds\": etfs.select(*etf_bond_columns),\n",
    "                \"holdings\": etfs.select(*etf_holding_columns),\n",
    "                \"returns_period\": etfs.select(*etf_return_columns),\n",
    "                \"returns_annual\": etfs.select(*etf_annual_return_columns),\n",
    "                \"risk\": etfs.select(*etf_risk_columns)\n",
    "            }\n",
    "\n",
    "            print(f\"Created {len(group_dfs)} grouped DataFrames by column pattern for ETFs.\")\n",
    "            return group_dfs\n",
    "\n",
    "    def clean_and_store(self, df: DataFrame, name: str, id_columns: list, output_path: str):\n",
    "        print(f\"Starting processing for group: {name}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Remove rows where all non-ID columns are null\n",
    "        cols_to_check = [c for c in df.columns if c not in id_columns]\n",
    "        df = df.dropna(subset=cols_to_check, how=\"all\")\n",
    "\n",
    "        # Remove rows with string values in non-ID columns\n",
    "        for c in cols_to_check:\n",
    "            df = df.filter(~col(c).rlike(\"[^0-9.,-]\"))\n",
    "\n",
    "        # Convert non-ID columns to DoubleType\n",
    "        for c in cols_to_check:\n",
    "            df = df.withColumn(c, F.regexp_replace(F.col(c), ',', ''))\n",
    "            df = df.withColumn(c, F.when(F.col(c).isin(\"NULL\", \"\"), None).otherwise(F.col(c).cast(DoubleType())))\n",
    "\n",
    "        non_double_columns = [f.name for f in df.schema.fields if not isinstance(f.dataType, DoubleType) and f.name not in id_columns]\n",
    "\n",
    "        for column in non_double_columns:\n",
    "            desc_vals = df.select(column).distinct().orderBy(F.col(column).desc()).limit(1000).rdd.map(lambda r: str(r[0])).collect()\n",
    "            asc_vals = df.select(column).distinct().orderBy(F.col(column).asc()).limit(1000).rdd.map(lambda r: str(r[0])).collect()\n",
    "\n",
    "            def is_mixed(vals):\n",
    "                return any(not v.replace('.', '', 1).replace('-', '', 1).isdigit() for v in vals)\n",
    "\n",
    "            if is_mixed(desc_vals) or is_mixed(asc_vals):\n",
    "                # Separate rows with string-like values\n",
    "                string_rows = df.filter(F.col(column).rlike(\"[^0-9.,-]\"))\n",
    "                fund_prices = df.filter(~F.col(column).rlike(\"[^0-9.,-]\"))\n",
    "                fund_prices = df.withColumn(column, F.col(column).cast(DoubleType()))\n",
    "\n",
    "                print(f\"\\nColumn '{column}' contained mixed values. String rows moved to separate DataFrame.\")\n",
    "                string_rows.select(column).distinct().show()\n",
    "        \n",
    "\n",
    "        # Add insertion_date column\n",
    "        df = df.withColumn(\"insertion_date\", current_date())\n",
    "\n",
    "        # Repartition for parallel writing\n",
    "        df = df.repartition(100)\n",
    "\n",
    "        # Cache if reused\n",
    "        df.cache()\n",
    "\n",
    "        # Write to DBFS partitioned by insertion_date\n",
    "        try: \n",
    "            output_dir = f\"{output_path}/{name}/\"\n",
    "            df.write.mode(\"overwrite\").partitionBy(\"insertion_date\").parquet(output_dir)\n",
    "        except Exception as e:\n",
    "            print(f'Error Processing group {name}, Exception {e}')\n",
    "\n",
    "        df.unpersist()\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Finished processing group: {name} in {end_time - start_time:.2f} seconds. Saved to {output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "    def run_etl_pipeline(self):\n",
    "\n",
    "        print('Importing Data')\n",
    "        self.import_data()\n",
    "        print('Ingesting data')\n",
    "        etf_prices, etfs, funds, fund_prices = self.ingest_data()\n",
    "\n",
    "        print('Loading treating dic etfs...')\n",
    "        group_dfs_etfs = self.transform(etfs, 'etfs')\n",
    "        print('Loading treating dic funds...')\n",
    "        group_dfs_funds = self.transform(funds, 'funds')\n",
    "\n",
    "        # ===== ETFS Transform and Loading\n",
    "        # ==================\n",
    "        print('Transforming ETFs')\n",
    "        etfs = etfs.repartition(200).cache()\n",
    "        etfs.count()  # to trigger caching immediately\n",
    "\n",
    "        # Apply function to each DataFrame\n",
    "        dbfs_output = \"dbfs:/tmp/silver/etfs\"\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(self.clean_and_store, df, name, id_columns[:2], dbfs_output)\n",
    "                for name, df in group_dfs_etfs.items()\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "        \n",
    "        etfs.unpersist()\n",
    "\n",
    "        # ======== FUNDS Transform and Loading\n",
    "        # ==================\n",
    "\n",
    "        print('Transforming Funds')\n",
    "        funds = funds.repartition(200).cache()\n",
    "        funds.count()  # to trigger caching immediately\n",
    "\n",
    "\n",
    "       # Apply function to each DataFrame\n",
    "        dbfs_output = \"dbfs:/tmp/silver/funds\"\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(self.clean_and_store, df, name, id_columns[:2], dbfs_output)\n",
    "                for name, df in group_dfs_funds.items()\n",
    "            ]\n",
    "            for future in futures:\n",
    "                future.result()\n",
    "        \n",
    "        funds.unpersist()\n",
    "\n",
    "        # ========== ETF_PRICES Transform and Loading\n",
    "        # =========================\n",
    "        print('Loading ETF PRICES')\n",
    "        etf_prices = etf_prices.withColumn(\"insertion_date\", current_date())\n",
    "\n",
    "        etf_prices = etf_prices.repartition(200).cache()\n",
    "        etf_prices.cache()\n",
    "\n",
    "        dbfs_output = \"dbfs:/tmp/silver/etf_prices\"\n",
    "\n",
    "        # Write to DBFS partitioned by insertion_date\n",
    "        try: \n",
    "            output_dir = f\"{dbfs_output}/\"\n",
    "            etf_prices.write.mode(\"overwrite\").partitionBy(\"insertion_date\").parquet(output_dir)\n",
    "        except Exception as e:\n",
    "            print(f'Error Processing group {name}, Exception {e}')\n",
    "\n",
    "        etf_prices.cache()\n",
    "        # ========== FUND_PRICES Transform and Loading\n",
    "        # =========================\n",
    "        print('Loading Fund Prices')\n",
    "        fund_prices = fund_prices.withColumn(\"insertion_date\", current_date())\n",
    "        dbfs_output = \"dbfs:/tmp/silver/fund_prices\"\n",
    "\n",
    "        fund_prices = fund_prices.repartition(200).cache()\n",
    "        fund_prices.cache()\n",
    "\n",
    "        # Write to DBFS partitioned by insertion_date\n",
    "        try: \n",
    "            output_dir = f\"{dbfs_output}/\"\n",
    "            fund_prices.write.mode(\"overwrite\").partitionBy(\"insertion_date\").parquet(output_dir)\n",
    "        except Exception as e:\n",
    "            print(f'Error Processing group {name}, Exception {e}')\n",
    "\n",
    "        fund_prices.cache()\n",
    "\n",
    "        print('Cleaning pipeline finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b571b5-4cc5-4ec4-95b4-e63802e35140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaning = Cleaning()\n",
    "cleaning.run_etl_pipeline()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
