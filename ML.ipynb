{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eac1c19c-5332-43c2-aa06-95df9a8c503c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction and Dataset Overview\n",
    "\n",
    "\n",
    "We chose a dataset about ETFs and Mutual Funds from Yahoo Finance. It includes lots of financial information and daily prices for thousands of funds. ETFs have become very popular because they cost less than mutual funds and are easier to manage, so predicting their daily returns is useful for investors.\n",
    "\n",
    "## What Makes This Dataset Interesting?\n",
    "\n",
    "- It has a large amount of data, which lets us show how to work with big data using PySpark and Databricks.\n",
    "- The data contains different types of financial information like prices, volumes, and returns, allowing us to explore, clean, and build models.\n",
    "- Predicting whether an ETF will go up or down each day is a real problem that can help with making better investment decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fc4940-8c0b-46d6-9800-4e655e4b2006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586359c0-b79b-44ec-972c-333dd50660ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETF Daily Return Prediction Project\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Exchange Traded Funds (ETFs) are popular investment vehicles that track indexes or baskets of assets.  \n",
    "Our goal is to **predict whether an ETF will have a positive daily return** — meaning if the closing price will be higher than the opening price on a given day — **before the market closes**.\n",
    "\n",
    "This is formulated as a **binary classification problem**, where the target variable indicates whether the ETF's price will increase during the trading day.\n",
    "\n",
    "## Important Clarification\n",
    "\n",
    "- While the dataset contains `open` and `close` prices for each day, **we cannot use the `close` price of the day to predict that day's return** because it would be data leakage (the value we want to predict would be an input).\n",
    "- Instead, the model should use **historical data and engineered features derived from previous days** (e.g., previous day returns, moving averages, volume trends) to predict whether the return will be positive for the next day.\n",
    "- The `close` price is only used to **label the data**, i.e., to compute if the return was positive or not for training and evaluation.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We use the **ETF prices** dataset, which contains daily historical prices for ETFs, including:\n",
    "\n",
    "- `fund_symbol`: the ETF ticker symbol\n",
    "- `price_date`: date of the price record\n",
    "- `open`, `close`, `adj_close`: daily prices (adjusted close accounts for dividends and splits)\n",
    "- `volume`: trading volume\n",
    "\n",
    "## Objective\n",
    "\n",
    "- Load and clean the data using PySpark.\n",
    "- Create a binary target column `daily_return_positive`:  \n",
    "  1 if `close > open` (the ETF gained value during the day), 0 otherwise.\n",
    "- Perform exploratory data analysis (EDA) and data quality checks.\n",
    "- Engineer features using historical price and volume data **prior to the day to be predicted**.\n",
    "- Prepare the dataset for machine learning using Spark MLlib.\n",
    "- Train and evaluate classification models to predict daily return direction *ahead of market close*.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Data cleaning (handling missing or zero prices, removing outliers).\n",
    "- Feature engineering (lagged returns, moving averages, volume changes).\n",
    "- Constructing the ML pipeline and model training.\n",
    "- Model evaluation, tuning, and interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "This project demonstrates skills in distributed data processing and machine learning with PySpark on Databricks, focusing on financial time series prediction while avoiding data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48dea9eb-e65c-4984-81a3-ea2a2a0451a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etf_prices = spark.read.parquet(\"dbfs:/tmp/silver/etf_prices\").filter(\"insertion_date = current_date()-1\")\n",
    "display(etf_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c65e18-451d-466f-899b-6248d99a966d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(etf_prices.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3b68076-7f6c-44a8-81c1-e222200bb4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dataset Summary and Quick Insights\n",
    "\n",
    "The dataset contains approximately 3.86 million rows of daily ETF price data, including opening, closing, adjusted closing prices, and traded volume.\n",
    "\n",
    "- **Fund Symbol:** Identifies each ETF (categorical, so no meaningful numeric statistics like mean or standard deviation).\n",
    "- **Price Variables (`open`, `close`, `adj_close`):**  \n",
    "  Average prices hover around 120,000, but extremely high standard deviations (~11 million) indicate the presence of significant outliers skewing the data.  \n",
    "  Minimum prices of zero are suspicious and likely represent missing or erroneous data since real ETF prices should not be zero.\n",
    "- **Volume:**  \n",
    "  Trading volume ranges dramatically, from 0 up to nearly 3 billion shares traded in a day. Zero volume may indicate days without trading or data issues. The average volume (~1 million) reflects typical daily activity but with high variability.\n",
    "- **Outliers and Inconsistencies:**  \n",
    "  The extreme maximum values in prices and volumes (reaching billions) suggest some data points may be extraordinary market events or more likely data errors needing cleaning.\n",
    "\n",
    "**Conclusion:**  \n",
    "Before any modeling or detailed analysis, the data requires thorough cleaning to handle zero and extreme values. Due to the dataset’s size and wide value ranges, sampling and scaling will be crucial for efficient processing and to ensure meaningful machine learning results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03d7d8e7-84c0-49d6-9fa7-9efe0a2b95dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filter out rows with zero or negative values in `open`, `close`, or `volume` as they likely represent invalid data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ff6f00-3a2c-45bc-89d6-ce6bbf47d20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned = etf_prices.filter(\n",
    "    (col(\"open\") > 0) &\n",
    "    (col(\"close\") > 0) &\n",
    "    (col(\"adj_close\") > 0) &\n",
    "    (col(\"volume\") > 0)\n",
    ").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e2290f-7fe2-4f13-8d66-0af945c7b3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "quantiles = cleaned.approxQuantile([\"open\", \"close\", \"volume\"], [0.01, 0.99], 0.01)\n",
    "open_low, open_high = quantiles[0]\n",
    "close_low, close_high = quantiles[1]\n",
    "vol_low, vol_high = quantiles[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f292d0fd-68d4-461e-a527-1af476060a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Winsorisation\n",
    "def winsorize(col_name, low, high):\n",
    "    return when(col(col_name) < low, lit(low)) \\\n",
    "           .when(col(col_name) > high, lit(high)) \\\n",
    "           .otherwise(col(col_name))\n",
    "\n",
    "cleaned = cleaned.withColumn(\"open_winsor\", winsorize(\"open\", open_low, open_high).cast(DoubleType())) \\\n",
    "                 .withColumn(\"close_winsor\", winsorize(\"close\", close_low, close_high).cast(DoubleType())) \\\n",
    "                 .withColumn(\"volume_winsor\", winsorize(\"volume\", vol_low, vol_high).cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a59a71d-cdc0-4b32-ac80-89358752fc19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned.withColumn(\"daily_return_positive\", (col(\"close_winsor\") > col(\"open_winsor\")).cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb95c67-b21d-4441-8a2d-bfc4bada37ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned.withColumn(\n",
    "    \"daily_return_pct\",\n",
    "    ((col(\"close_winsor\") - col(\"open_winsor\")) / col(\"open_winsor\")) * 100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5d1ab93-9854-41ad-9007-4279e1281095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned.select(\"fund_symbol\", \"price_date\", \"open_winsor\", \"close_winsor\", \"volume_winsor\", \"daily_return_positive\", \"daily_return_pct\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af138259-18ef-455f-b9a4-2c43ebaa792e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned.drop(\"open\", \"close\", \"volume\")\n",
    "\n",
    "# Rename winsorized columns to original names\n",
    "cleaned = cleaned.withColumnRenamed(\"open_winsor\", \"open\") \\\n",
    "                 .withColumnRenamed(\"close_winsor\", \"close\") \\\n",
    "                 .withColumnRenamed(\"volume_winsor\", \"volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcba0b1a-cc70-4542-9142-663c505e27d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e918b847-d3b0-4f7e-bffa-0b9a9231de0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Revisiting Outlier Treatment: Beyond Winsorization\n",
    "\n",
    "While we previously applied **winsorization at the 1st and 99th percentiles** to limit the impact of extreme values on `open`, `close`, and `volume`, this approach proved **insufficient for the `daily_return_pct`** variable.\n",
    "\n",
    "#### Issues Identified:\n",
    "- From the dataset summary:\n",
    "  - `daily_return_pct` has a **maximum of over 364%** and a **minimum near -98%**, which is abnormally wide for a daily return.\n",
    "  - The **standard deviation is ~1.42**, which is disproportionately large compared to the median (0%), indicating **high skew and heavy tails**.\n",
    "- These extreme values may result from **splits, errors, or rare market events**, and will likely **harm model training** by introducing noise or misleading variance.\n",
    "\n",
    "#### Solution:\n",
    "We will now apply a **more robust outlier filtering method** using:\n",
    "- The **IQR (Interquartile Range)** method to detect and remove statistical outliers.\n",
    "- A **hard cap at ±100%** daily return to eliminate unrealistic values.\n",
    "\n",
    "This ensures a more realistic and stable data distribution for classification modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127db3e2-6bb7-4da4-a664-253b516a3044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "def iqr_winsorize(df, col_name, absolute_cap=None):\n",
    "    q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "    if absolute_cap is not None:\n",
    "        upper_bound = min(upper_bound, absolute_cap)\n",
    "\n",
    "    return df.withColumn(\n",
    "        f\"{col_name}_winsor\",\n",
    "        when(col(col_name) < lower_bound, lower_bound)\n",
    "        .when(col(col_name) > upper_bound, upper_bound)\n",
    "        .otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Appliquer la winsorisation sur les colonnes concernées\n",
    "cleaned_filtered = cleaned\n",
    "cleaned_filtered = iqr_winsorize(cleaned_filtered, \"daily_return_pct\", absolute_cap=100.0)\n",
    "cleaned_filtered = iqr_winsorize(cleaned_filtered, \"open\", absolute_cap=1000)\n",
    "cleaned_filtered = iqr_winsorize(cleaned_filtered, \"close\", absolute_cap=1000)\n",
    "cleaned_filtered = iqr_winsorize(cleaned_filtered, \"volume\", absolute_cap=1_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82954572-9fa5-4faa-bd7e-399b8d26eb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_filtered.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5de7ad-6856-499a-b6df-7f005c5a4d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Drop original columns\n",
    "cols_to_drop = [\"open\", \"close\", \"volume\", \"daily_return_positive\", \"daily_return_pct\",\"adj_close\",\"daily_return_pct_winsor\"]\n",
    "cleaned_filtered = cleaned_filtered.drop(*cols_to_drop)\n",
    "\n",
    "# Recalculate daily_return_pct from winsorized open and close\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"daily_return_pct\",\n",
    "    (col(\"close_winsor\") - col(\"open_winsor\")) / col(\"open_winsor\") * 100\n",
    ")\n",
    "\n",
    "# Recalculate daily_return_positive from new daily_return_pct\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"daily_return_positive\",\n",
    "    when(col(\"daily_return_pct\") > 0, 1).otherwise(0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42996b3b-a90d-494b-a5fb-f95b7488122a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_filtered.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec73daea-0580-42b7-afa6-c2974cc777fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Updated Summary After Winsorization\n",
    "\n",
    "The dataset after winsorization shows a much-improved distribution across key financial variables:\n",
    "\n",
    "- **Price columns (`open_winsor`, `close_winsor`)**:\n",
    "  - Mean prices around 43 USD, with medians near 34 USD.\n",
    "  - The maximum prices are capped below 100, effectively removing extreme outliers.\n",
    "  - The standard deviation (~25) indicates moderate variability, which is reasonable for market prices.\n",
    "\n",
    "- **Volume (`volume_winsor`)**:\n",
    "  - Mean volume is approximately 93,000 shares.\n",
    "  - Median volume is 24,600 shares, with a 75th percentile at 138,600, showing a realistic distribution of trading activity.\n",
    "  - Maximum volume is limited to about 342,550, avoiding the impact of unrealistic spikes.\n",
    "\n",
    "- **Daily Returns (`daily_return_pct`)**:\n",
    "  - Mean daily return is close to zero (0.058%), reflecting a balanced market movement.\n",
    "  - Returns range from approximately -68% to +360%, showing that some extreme but plausible moves remain after winsorization.\n",
    "  - Median and 75th percentile returns are modest (3.36% and 56.12%), indicating typical daily fluctuations.\n",
    "\n",
    "- **Daily Return Positive Flag (`daily_return_positive`)**:\n",
    "  - Approximately 51% of the daily returns are positive, reflecting a balanced distribution of gains and losses.\n",
    "\n",
    "Overall, the winsorization and outlier handling have effectively reduced extreme anomalies while preserving meaningful market dynamics, making this dataset suitable for robust analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5abd77-7a3d-4178-8cd7-29e1a91a7c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleaned_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49937663-d24f-43cf-975e-e1c59213eb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Top 10 Days with Highest Daily Returns\n",
    "\n",
    "Show the days and ETFs with the highest positive returns as an example of useful exploratory insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f7d3c7-d846-4250-a786-5eb4620ef6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "cleaned_filtered.orderBy(desc(\"daily_return_pct\")) \\\n",
    "                .select(\"fund_symbol\", \"price_date\", \"daily_return_pct\") \\\n",
    "                .show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd642680-b981-46a0-aa6b-19a807f10380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Despite winsorization, some assets still exhibit extremely high daily returns (above 100%), mainly leveraged ETFs and volatile securities. These extreme values can heavily skew model training, potentially leading to overfitting or unstable predictions.\n",
    "\n",
    "**Financially sensible approach:**\n",
    "\n",
    "- **Cap daily returns at ±100%** to exclude unrealistic or excessively volatile spikes that are unlikely to represent typical market behavior.\n",
    "- This threshold balances keeping meaningful volatility signals without letting extreme outliers dominate.\n",
    "- It aligns with risk management practices by preventing models from reacting disproportionately to rare and extreme events.\n",
    "\n",
    "By applying this hard cap, we ensure a cleaner dataset, more stable model training, and better generalization for typical market conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037184aa-326b-454b-adc2-976a8fa54db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sample_pd = cleaned_filtered.select(\n",
    "    \"daily_return_pct\", \"open_winsor\", \"close_winsor\", \"volume_winsor\"\n",
    ").sample(fraction=0.01, seed=42).toPandas()  \n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(sample_pd[\"daily_return_pct\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of daily_return_pct (winsorized)\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(sample_pd[\"open_winsor\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of open price (winsorized)\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(sample_pd[\"close_winsor\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of close price (winsorized)\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(sample_pd[\"volume_winsor\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of volume (winsorized)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a85f41b3-d6e9-4973-b55c-ce750402775d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Analysis Summary\n",
    "\n",
    "After applying winsorization and capping, the dataset shows a much cleaner and business-relevant distribution.\n",
    "\n",
    "- **Daily Returns (`daily_return_pct`)**:  \n",
    "  The median return is approximately 0.03%, with an interquartile range between -0.28% and +0.56%. Capping extreme values at 100% reduces the influence of rare, highly volatile sessions — common with leveraged or niche ETFs. Around 51% of returns are positive, indicating balanced market movements.\n",
    "\n",
    "- **Prices (`open_winsor`, `close_winsor`)**:  \n",
    "  Median prices are around 34 USD, with a max below 100 USD — realistic for well-known ETFs and mid-range equities. The cap excludes ultra-high-priced stocks like Google or Amazon, focusing instead on widely traded instruments. Tickers such as AAA (Investment Grade Bond ETF) and AAAU (Gold Trust ETF) confirm the dataset includes legitimate, everyday financial assets. Price variability (std ≈ 26 USD) remains within reasonable bounds.\n",
    "\n",
    "- **Volume (`volume_winsor`)**:  \n",
    "  The median daily volume is about 24,600 shares, with an upper limit of ~342,000, helping to eliminate outliers without discarding active trading patterns.\n",
    "\n",
    "**Business context:**  \n",
    "This cleaned dataset retains credible financial behaviors while removing anomalies, making it robust for training predictive models with reduced risk of bias or overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3d6c3e-4049-437d-a601-5a22a941b5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, mean, min, max\n",
    "\n",
    "fund_summary = cleaned_filtered.groupBy(\"fund_symbol\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    mean(\"daily_return_pct_capped\").alias(\"mean_return_pct\"),\n",
    "    min(\"daily_return_pct_capped\").alias(\"min_return_pct\"),\n",
    "    max(\"daily_return_pct_capped\").alias(\"max_return_pct\"),\n",
    "    \n",
    "    mean(\"open_winsor\").alias(\"mean_open\"),\n",
    "    min(\"open_winsor\").alias(\"min_open\"),\n",
    "    max(\"open_winsor\").alias(\"max_open\"),\n",
    "    \n",
    "    mean(\"close_winsor\").alias(\"mean_close\"),\n",
    "    min(\"close_winsor\").alias(\"min_close\"),\n",
    "    max(\"close_winsor\").alias(\"max_close\"),\n",
    "    \n",
    "    mean(\"volume_winsor\").alias(\"mean_volume\"),\n",
    "    min(\"volume_winsor\").alias(\"min_volume\"),\n",
    "    max(\"volume_winsor\").alias(\"max_volume\")\n",
    ")\n",
    "\n",
    "fund_summary.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13eb5fef-7d68-44a0-8d0e-2847625c266d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fund-Level Summary (top 10 rows)\n",
    "\n",
    "Based on the cleaned and capped dataset:\n",
    "\n",
    "- **Returns**:\n",
    "  - Most funds show average daily returns close to 0%, indicating stable performance.\n",
    "  - Some like `BBEU` have a positive average return (~0.07%), suggesting consistent upward movement.\n",
    "  - Others like `AWAY` and `BJK` show slightly negative trends, possibly reflecting sector volatility.\n",
    "\n",
    "- **Prices**:\n",
    "  - Median open/close prices vary: `AESR` and `CHII` are low-cost funds ($11–13), while `BBEU` and `BJK` are more expensive ($36–50).\n",
    "  - Price max caps below $100 ensure focus on commonly traded ETFs.\n",
    "\n",
    "- **Volume**:\n",
    "  - Significant variability: some funds like `BBEU` trade with high average volume (~227k), others like `BSMT` or `AQWA` have low liquidity (<10k).\n",
    "  - This affects model reliability—higher volume means more stable price behavior.\n",
    "\n",
    "This summary helps identify fund characteristics (growth vs. stability, liquid vs. illiquid) for modeling or investment strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06e4de8-696a-4a1e-a80f-2771a5a6b85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Feature Engineering\n",
    "\n",
    "We now enrich our dataset with new features to improve the modeling phase.\n",
    "\n",
    "The following engineered features are added:\n",
    "\n",
    "- `price_diff`: Absolute difference between close and open prices (no ratio, to avoid outlier inflation).\n",
    "- `volatility_proxy`: Proxy for volatility measured as the absolute value of capped daily return percentage.\n",
    "- `volume_log`: Log-transformed volume to handle large scale differences and reduce skew.\n",
    "- `is_high_volume`: Flag if the day's volume is in the top 25% quantile for that fund.\n",
    "- `price_avg`: Average of open and close prices — useful as a smoothed price indicator.\n",
    "\n",
    "These features aim to capture volatility, liquidity, and trend signals without introducing extreme values. They are computed using PySpark's parallelized operations to handle large-scale data efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ecafac3-36c6-4a0b-8686-0dd286518b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, log1p, lit, avg, col, when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Absolute price difference (close - open)\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"price_diff\", abs(col(\"close_winsor\") - col(\"open_winsor\"))\n",
    ")\n",
    "\n",
    "# Volatility proxy — absolute daily return percentage\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"volatility_proxy\", abs(col(\"daily_return_pct_capped\"))\n",
    ")\n",
    "\n",
    "# Log-transformed volume to reduce skew\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"volume_log\", log1p(col(\"volume_winsor\"))\n",
    ")\n",
    "\n",
    "# Price average (close + open) / 2 — a smoother price signal\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"price_avg\", (col(\"open_winsor\") + col(\"close_winsor\")) / 2\n",
    ")\n",
    "\n",
    "# High volume flag — if volume is in the top 25% for that fund\n",
    "volume_window = Window.partitionBy(\"fund_symbol\")\n",
    "volume_75 = cleaned_filtered.approxQuantile(\"volume_winsor\", [0.75], 0.01)[0]\n",
    "\n",
    "cleaned_filtered = cleaned_filtered.withColumn(\n",
    "    \"is_high_volume\",\n",
    "    when(col(\"volume_winsor\") >= volume_75, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Show result\n",
    "cleaned_filtered.select(\"fund_symbol\", \"price_diff\", \"volatility_proxy\", \"volume_log\", \"price_avg\", \"is_high_volume\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1eecb9c-6f82-41da-aaa6-f642e27b3e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The average daily price difference is small, around 0.01 to 0.03 USD, indicating stable intraday price movement typical for liquid assets. The volatility proxy captures realistic price fluctuations without extreme spikes, and the log-transformed volume balances varying trading activity levels. About 14% of days are classified as high volume, useful for distinguishing active trading periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fe6303-3a23-44c2-94ea-70f71346a33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modeling Approach\n",
    "\n",
    "We aim to predict if an ETF will have a positive daily return (binary classification).  \n",
    "Key points:\n",
    "\n",
    "- Target: `daily_return_positive` (1 if next day's close > open, else 0)  \n",
    "- Features: engineered from historical data *before* the prediction day (lags, moving averages, volume trends etc.)  \n",
    "- Model: Use Spark MLlib’s classification algorithms (Logistic Regression, Random Forest, or Gradient Boosted Trees)  \n",
    "- Pipeline: Feature assembler + model + evaluator  \n",
    "- Evaluation: Use area under ROC curve (AUC), accuracy, and F1-score  \n",
    "- Train/test split: Time-aware split (train on past dates, test on later dates to avoid leakage)  \n",
    "\n",
    "This ensures no future data leaks into training, respecting the financial prediction constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77e6036-8696-47de-b51c-dd5559951560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use engineered features only (exclude anything that leaks info about the same day's close price)\n",
    "feature_cols = [\n",
    "    \"price_diff\",         # open_winsor - close_winsor of previous day or engineered lag\n",
    "    \"volatility_proxy\",   # rolling volatility proxy from previous days\n",
    "    \"volume_log\",         # log of volume, lagged or winsorized\n",
    "    \"price_avg\",          # average price feature\n",
    "    \"is_high_volume\"      # volume indicator (binary)\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"daily_return_positive\",  # or use daily_return_positive_capped if preferred\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Split respecting time (train on earlier dates, test on later dates)\n",
    "train_df = cleaned_filtered.filter(\"price_date < '2021-01-01'\")\n",
    "test_df = cleaned_filtered.filter(\"price_date >= '2021-01-01'\")\n",
    "\n",
    "# Fit model on training set\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Predict on test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate with AUC (area under ROC curve)\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"daily_return_positive\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce958876-cbae-4765-9e56-55a25b689214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Comparison and Hyperparameter Optimization\n",
    "\n",
    "To improve our ETF daily return prediction (for now, AUC = 0.68), we test different classification algorithms available in Spark MLlib:  \n",
    "- Logistic Regression (baseline linear model)  \n",
    "- Random Forest (already tested)  \n",
    "- Gradient Boosted Trees (powerful ensemble method)  \n",
    "\n",
    "Then, we perform hyperparameter tuning using CrossValidator with a simple grid search on key parameters (e.g., maxDepth, numTrees) to improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8ff041-9f11-41cc-9a84-13f9d11af066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Prepare features vector as before\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Define classifiers to test\n",
    "lr = LogisticRegression(labelCol=\"daily_return_positive\", featuresCol=\"features\", maxIter=20)\n",
    "rf = RandomForestClassifier(labelCol=\"daily_return_positive\", featuresCol=\"features\", seed=42)\n",
    "gbt = GBTClassifier(labelCol=\"daily_return_positive\", featuresCol=\"features\", maxIter=20, seed=42)\n",
    "\n",
    "# Create pipelines for each\n",
    "pipeline_lr = Pipeline(stages=[assembler, lr])\n",
    "pipeline_rf = Pipeline(stages=[assembler, rf])\n",
    "pipeline_gbt = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Train-test split (same as before)\n",
    "train_df = cleaned_filtered.filter(\"price_date < '2021-01-01'\")\n",
    "test_df = cleaned_filtered.filter(\"price_date >= '2021-01-01'\")\n",
    "\n",
    "# Fit and evaluate each model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"daily_return_positive\", metricName=\"areaUnderROC\")\n",
    "\n",
    "def train_evaluate(pipeline, train_data, test_data, model_name):\n",
    "    model = pipeline.fit(train_data)\n",
    "    preds = model.transform(test_data)\n",
    "    auc = evaluator.evaluate(preds)\n",
    "    print(f\"{model_name} Test AUC: {auc:.4f}\")\n",
    "    return model, auc\n",
    "\n",
    "print(\"Training and evaluating baseline models...\")\n",
    "lr_model, lr_auc = train_evaluate(pipeline_lr, train_df, test_df, \"Logistic Regression\")\n",
    "rf_model, rf_auc = train_evaluate(pipeline_rf, train_df, test_df, \"Random Forest\")\n",
    "gbt_model, gbt_auc = train_evaluate(pipeline_gbt, train_df, test_df, \"Gradient Boosted Trees\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest (example)\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [20, 50, 100])\\\n",
    "    .addGrid(rf.maxDepth, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3,  # 3-fold cross validation\n",
    "                          parallelism=4)  # Parallelism for faster tuning\n",
    "\n",
    "print(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# Evaluate best model on test data\n",
    "best_model = cv_model.bestModel\n",
    "predictions = best_model.transform(test_df)\n",
    "best_auc = evaluator.evaluate(predictions)\n",
    "print(f\"Best Random Forest after tuning Test AUC: {best_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5053e7a5-b06b-4193-8701-3692ee1faf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, we explored a large ETF financial dataset using PySpark, demonstrating key skills in big data processing, feature engineering, and machine learning.\n",
    "\n",
    "We built several classification models to predict whether an ETF’s price would increase during the trading day, achieving reasonable performance with a test AUC around 0.68 for tree-based models. Despite efforts to tune hyperparameters, model improvements were limited, suggesting that more complex features or external data may be needed for significant gains.\n",
    "\n",
    "Overall, this project highlights the challenges and opportunities of applying big data techniques to financial time series. The use of PySpark enabled efficient handling of large datasets, and the predictive models provide a solid baseline for future improvements.\n",
    "\n",
    "This experience underscores the importance of careful feature design and the potential of scalable tools like Spark in real-world data science problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1be44fc-212c-449f-9065-5b2db91d5220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
